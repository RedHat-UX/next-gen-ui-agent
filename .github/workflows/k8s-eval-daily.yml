name: K8s Evaluation Tests

on:
  # Run on push to feature branch
  push:
    branches:
      - "test_ai_eval"

  # Run on pull request
  pull_request:
    branches:
      - "main"

  # Run daily at 2 AM UTC (NOTE: Only runs on default branch 'main' after merge)
  # schedule:
  #   - cron: "0 2 * * *"

  # Allow manual trigger (works on any branch!)
  workflow_dispatch:

jobs:
  k8s-eval:
    name: Run K8s Evaluation Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install agent core dependencies
          pip install -r libs/3rdparty/python/agent-core-requirements.txt
          # Install llama-stack with specific version constraint
          pip install -r libs/3rdparty/python/llama-stack-requirements.txt
          pip install -c libs/3rdparty/python/llama-stack-client-constraints.txt \
            -r libs/3rdparty/python/llama-stack-client-requirements.txt

      - name: Install Ollama (for LLM)
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          # Start Ollama in background
          ollama serve &
          # Wait for Ollama to be ready
          for i in {1..30}; do
            if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
              echo "Ollama is ready!"
              break
            fi
            echo "Waiting for Ollama to start... ($i/30)"
            sleep 2
          done
          # Pull the model
          ollama pull llama3.2:3b

      - name: Run K8s evaluation tests
        run: |
          # Run from repo root with proper PYTHONPATH (same as local testing)
          PYTHONPATH=tests:libs INFERENCE_MODEL=llama3.2:3b \
            python tests/ai_eval_components/eval_k8s.py

      - name: Generate HTML report
        if: always()
        run: |
          python tests/ai_eval_components/generate_k8s_report.py

      - name: Upload evaluation report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: k8s-eval-report
          path: tests/ai_eval_components/k8s_eval_report.html
          retention-days: 30

      - name: Upload error logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: k8s-eval-errors
          path: tests/ai_eval_components/errors/
          retention-days: 7
