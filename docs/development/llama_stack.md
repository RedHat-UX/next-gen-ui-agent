# Llama Stack Server for development purposes

Few Next Gen UI Agent modules use [Llama Stack](https://llama-stack.readthedocs.io) server for LLM inference abstraction. 

For local developmment purposes, you can run Llama Stack server on localhost, configured to server LLM of your choice. 
You can also rul LLM locally using eg. [Ollama](https://ollama.com/) if you have reasonable HW. It is definitelly 
good to have GPU capable to accelerate AI worloads.

For more details see [LLAMASTACK_DEV.md](https://github.com/RedHat-UX/next-gen-ui-agent/blob/main/LLAMASTACK_DEV.md).
