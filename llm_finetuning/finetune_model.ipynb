{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune LLM for Next Gen UI Agent using Unsloth and LoRA\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Fine-tune a 3B or 8B parameter model using LoRA (Low-Rank Adaptation)\n",
        "2. Load training data from a simple JSON file (`question`/`answer` format) or multiple files\n",
        "3. Export the model in GGUF format for use with Ollama\n",
        "\n",
        "**Model**: Llama-3.2-3B-Instruct (you can also use Qwen2.5-3B-Instruct or similar)\n",
        "\n",
        "**Data Format**: Simple JSON with `question` and `answer` fields\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANT: GPU Required\n",
        "\n",
        "**This notebook requires a GPU to run!**\n",
        "\n",
        "Before you start, make sure GPU is enabled:\n",
        "1. Click **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **T4 GPU** (or better) under Hardware accelerator\n",
        "3. Click **Save**\n",
        "\n",
        "The notebook will check for GPU availability in Step 1 before installing dependencies.\n",
        "\n",
        "**Note:** TPU is NOT supported. This notebook requires NVIDIA CUDA GPUs only (Unsloth and bitsandbytes do not support TPUs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Verify GPU Availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"‚ùå ERROR: No GPU detected!\")\n",
        "    print(\"\\nüìã To enable GPU in Google Colab:\")\n",
        "    print(\"   1. Click 'Runtime' in the menu\")\n",
        "    print(\"   2. Select 'Change runtime type'\")\n",
        "    print(\"   3. Choose 'T4 GPU' under 'Hardware accelerator'\")\n",
        "    print(\"   4. Click 'Save'\")\n",
        "    print(\"   5. Re-run all cells from the beginning\")\n",
        "    raise RuntimeError(\"GPU is required for this notebook. Please enable GPU and restart.\")\n",
        "else:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"‚úÖ GPU detected: {gpu_name}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Required Packages\n",
        "\n",
        "**Note:** This step installs all dependencies. It may take 3-5 minutes. If you get xformers errors, don't worry - there's a fallback option below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth and dependencies\n",
        "print(\"Installing Unsloth and required packages...\")\n",
        "print(\"This may take 3-5 minutes...\\n\")\n",
        "\n",
        "# Install Unsloth\n",
        "%pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install other dependencies without building from source\n",
        "%pip install -q --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# Install xformers - use pre-built wheel (no building from source)\n",
        "print(\"\\nInstalling xformers...\")\n",
        "%pip install -q xformers --no-build-isolation\n",
        "\n",
        "print(\"\\n‚úÖ Installation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative Installation (If xformers fails)\n",
        "\n",
        "**Only run this cell if the above installation failed with xformers errors.**\n",
        "\n",
        "Xformers is optional and provides a small speed boost, but the notebook works without it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative installation without xformers\n",
        "# Uncomment the lines below ONLY if you had xformers errors above\n",
        "\n",
        "# print(\"Installing without xformers (optional speedup)...\")\n",
        "# %pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# %pip install -q --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "# print(\"‚úÖ Installation complete (without xformers)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "max_seq_length = 2048  # Choose any! Unsloth supports RoPE Scaling internally\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
        "\n",
        "# LoRA configuration\n",
        "lora_r = 16  # LoRA rank\n",
        "lora_alpha = 16  # LoRA alpha (scaling factor)\n",
        "lora_dropout = 0  # Dropout for LoRA layers\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\"]  # Modules to apply LoRA\n",
        "\n",
        "# Training configuration\n",
        "output_dir = \"./output\"\n",
        "num_train_epochs = 3\n",
        "per_device_train_batch_size = 2\n",
        "gradient_accumulation_steps = 4\n",
        "learning_rate = 2e-4\n",
        "warmup_steps = 5\n",
        "logging_steps = 1\n",
        "save_steps = 50\n",
        "\n",
        "# Model selection (choose one)\n",
        "model_name = \"unsloth/Llama-3.2-3B-Instruct\" # 3B model\n",
        "\n",
        "# Alternative 3B models:\n",
        "# model_name = \"unsloth/Qwen2.5-3B-Instruct\"  # 3B model\n",
        "# model_name = \"unsloth/Phi-3.5-mini-instruct\"  # 3.8B model\n",
        "\n",
        "# Alternative 8B models:\n",
        "# model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\" # 8B model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load Pre-trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer with Unsloth optimizations\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Configure LoRA Adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add LoRA adapters to the model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_r,\n",
        "    target_modules=target_modules,\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Efficient gradient checkpointing\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters configured!\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Upload and Load Training Data\n",
        "\n",
        "### Uploading Your Dataset\n",
        "\n",
        "**Manual Upload Instructions:**\n",
        "1. In Colab, click the **folder icon** (üìÅ) in the left sidebar to open the file browser\n",
        "2. Create a folder for your training data named `training_data` by right-clicking root directory ‚Üí `New Folder`\n",
        "3. Upload your JSON files into this folder by right-clicking the folder ‚Üí `Upload`\n",
        "4. You can upload multiple JSON files - they will be automatically combined and shuffled\n",
        "\n",
        "**Alternative:** Upload a single `training_data.json` file to the root directory\n",
        "\n",
        "**Required JSON format for each file:**\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"question\": \"What is machine learning?\",\n",
        "    \"answer\": \"Machine learning is a branch of AI that...\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What is deep learning?\",\n",
        "    \"answer\": \"Deep learning is a subset of machine learning...\"\n",
        "  }\n",
        "]\n",
        "```\n",
        "\n",
        "The notebook will load all JSON files from your specified folder and shuffle them before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the path to your training data\n",
        "# Option 1: Load from a folder containing multiple JSON files (recommended)\n",
        "data_path = \"training_data\"  # Folder containing your JSON files\n",
        "\n",
        "# Option 2: Load from a single JSON file (uncomment to use)\n",
        "# data_path = \"training_data.json\"\n",
        "\n",
        "print(f\"Loading training data from: {data_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data from JSON file(s)\n",
        "import glob\n",
        "import random\n",
        "\n",
        "training_data = []\n",
        "\n",
        "# Check if path is a directory or a file\n",
        "if os.path.isdir(data_path):\n",
        "    # Load all JSON files from the directory\n",
        "    json_files = glob.glob(os.path.join(data_path, \"*.json\"))\n",
        "    \n",
        "    if not json_files:\n",
        "        raise ValueError(f\"No JSON files found in directory: {data_path}\")\n",
        "    \n",
        "    print(f\"Found {len(json_files)} JSON file(s) in {data_path}/\")\n",
        "    \n",
        "    for json_file in sorted(json_files):\n",
        "        print(f\"  Loading: {os.path.basename(json_file)}\")\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            if isinstance(data, list):\n",
        "                training_data.extend(data)\n",
        "                print(f\"    Added {len(data)} examples\")\n",
        "            else:\n",
        "                raise ValueError(f\"File {json_file} must contain a JSON array\")\n",
        "    \n",
        "elif os.path.isfile(data_path):\n",
        "    # Load single JSON file\n",
        "    print(f\"Loading single file: {data_path}\")\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        training_data = json.load(f)\n",
        "        if not isinstance(training_data, list):\n",
        "            raise ValueError(f\"File {data_path} must contain a JSON array\")\n",
        "else:\n",
        "    raise ValueError(f\"Path not found: {data_path}\")\n",
        "\n",
        "# Validate data format\n",
        "if not training_data:\n",
        "    raise ValueError(\"Training data is empty!\")\n",
        "\n",
        "# Check first example has required fields\n",
        "first_example = training_data[0]\n",
        "if \"question\" not in first_example or \"answer\" not in first_example:\n",
        "    raise ValueError(\"Training data must have 'question' and 'answer' fields!\")\n",
        "\n",
        "# Shuffle the combined data to mix examples from different files\n",
        "print(f\"\\nShuffling {len(training_data)} training examples...\")\n",
        "random.seed(3407)  # Set seed for reproducibility\n",
        "random.shuffle(training_data)\n",
        "\n",
        "print(f\"‚úÖ Loaded and shuffled {len(training_data)} training examples\")\n",
        "print(\"\\nFirst example after shuffle:\")\n",
        "print(json.dumps(training_data[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Format Training Data\n",
        "\n",
        "**Note:** This notebook uses the tokenizer's `apply_chat_template()` method to format prompts. This makes the code model-agnostic and automatically adapts to any model's expected chat format (ChatML, Llama, Alpaca, etc.). No hardcoded format tags needed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format the data for instruction tuning using tokenizer's chat template\n",
        "# Required JSON format: [{\"question\": \"...\", \"answer\": \"...\"}]\n",
        "\n",
        "def format_prompt(example):\n",
        "    \"\"\"\n",
        "    Format a single example into a prompt using the tokenizer's chat template.\n",
        "    This makes the notebook work with any model automatically.\n",
        "    \n",
        "    Args:\n",
        "        example: Dict with 'question' and 'answer' keys\n",
        "        \n",
        "    Returns:\n",
        "        Dict with 'text' key containing the formatted prompt\n",
        "    \"\"\"\n",
        "    # Extract question and answer\n",
        "    try:\n",
        "        question = example[\"question\"]\n",
        "        answer = example[\"answer\"]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Example missing required field: {e}. Required: 'question' and 'answer'\")\n",
        "    \n",
        "    # Create conversation in standard format\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful UI design assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "        {\"role\": \"assistant\", \"content\": answer}\n",
        "    ]\n",
        "    \n",
        "    # Use tokenizer's chat template (works with any model)\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        conversation,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    \n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Convert to HuggingFace Dataset format\n",
        "print(\"Formatting training data...\")\n",
        "formatted_data = [format_prompt(example) for example in training_data]\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "print(f\"‚úÖ Dataset created with {len(dataset)} examples\")\n",
        "print(\"\\nExample formatted prompt:\")\n",
        "print(dataset[0]['text'][:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Configure Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    warmup_steps=warmup_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=logging_steps,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\", # Disables wandb/tensorboard prompts\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Trainer configured and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"Training loss: {trainer_stats.metrics['train_loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Test the Fine-tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample prompt\n",
        "FastLanguageModel.for_inference(model)  # Enable inference mode\n",
        "\n",
        "test_question = \"The data is an array with fifteen k8s pod objects with pod name, pod creation date, CPU utilization and consumed memory. User wants to see these data. What UI component to use?\"\n",
        "#test_question = \"The data is an array with fifteen k8s pod objects with pod name, pod creation date, CPU utilization and consumed memory. User is browsing them to get overview. What UI component to use?\"\n",
        "#test_question = \"The data is an array with fifteen k8s pod objects with pod name, pod creation date, CPU utilization and consumed memory. User is browsing them to get basic overview. What type of UI component to use?\"\n",
        "#test_question = \"The data is an array with fifteen k8s pod objects with pod name, pod creation date, image url containing CPU utilization and consumed memory info. User is browsing them to get basic overview. What type of UI component to use?\"\n",
        "\n",
        "# Create test conversation using tokenizer's chat template\n",
        "test_conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful UI design assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": test_question}\n",
        "]\n",
        "\n",
        "# Format using tokenizer's chat template\n",
        "test_prompt = tokenizer.apply_chat_template(\n",
        "    test_conversation,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True  # Adds the assistant prompt at the end\n",
        ")\n",
        "\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Test question:\")\n",
        "print(test_question)\n",
        "print(\"\\nModel response:\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Save the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model and LoRA adapters\n",
        "model_save_path = \"./finetuned_model\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Export to GGUF for Ollama\n",
        "\n",
        "This step exports the model directly to GGUF format for Ollama. The LoRA adapters are automatically merged during export.\n",
        "\n",
        "**Note:** This step (merging weights, `llama.cpp` and GGUF packages installation, final model quantization) takes around 20min!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save merged model in 16-bit precision\n",
        "# Uncomment this section ONLY if you need the merged model for:\n",
        "# - Uploading to HuggingFace Hub\n",
        "# - Using with other tools (vLLM, TGI, etc.)\n",
        "# - Testing before GGUF conversion\n",
        "# \n",
        "# For Ollama-only workflow, this step is NOT needed (GGUF export merges automatically)\n",
        "\n",
        "# merged_model_path = \"./merged_model_16bit\"\n",
        "# model.save_pretrained_merged(\n",
        "#     merged_model_path,\n",
        "#     tokenizer,\n",
        "#     save_method=\"merged_16bit\",  # Can also use \"merged_4bit\" for smaller size\n",
        "# )\n",
        "# print(f\"Merged 16-bit model saved to {merged_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to GGUF format for Ollama\n",
        "# Generate filename based on base model: NGUI-{model}.{quant_method}.gguf\n",
        "model_basename = model_name.split('/')[-1].lower()  # e.g., \"llama-3.2-3b-instruct\"\n",
        "model_basename = model_basename.replace('-instruct', '').replace('instruct', '').strip('-')\n",
        "\n",
        "print(f\"Base model: {model_basename}\")\n",
        "\n",
        "# Export `q4_k_m` quantization. Other quantization options `q5_k_m`, `q8_0`, `f16`\n",
        "quant_method = \"q4_k_m\"\n",
        "print(f\"\\nExporting to GGUF format with {quant_method} quantization...\")\n",
        "\n",
        "# save_pretrained_gguf saves the file in the current directory with the model's name\n",
        "# The first parameter is just a directory name that gets created (but file is in root)\n",
        "gguf_dir = \"gguf_output\"\n",
        "model.save_pretrained_gguf(\n",
        "    gguf_dir,\n",
        "    tokenizer,\n",
        "    quantization_method=quant_method,\n",
        ")\n",
        "\n",
        "# Find the generated GGUF file in the root directory\n",
        "import glob\n",
        "import shutil\n",
        "gguf_files = glob.glob(\"./*.gguf\")\n",
        "if gguf_files:\n",
        "    generated_file = gguf_files[0]  # e.g., \"./llama-3.2-3b-instruct.Q4_K_M.gguf\"\n",
        "    \n",
        "    # Rename to our desired format: NGUI-{model}.{quant_method}.gguf\n",
        "    gguf_filename = f\"NGUI-{model_basename}.{quant_method}.gguf\"\n",
        "    gguf_path = f\"./{gguf_filename}\"\n",
        "    \n",
        "    # Rename the file if it's not already named correctly\n",
        "    if generated_file != gguf_path:\n",
        "        shutil.move(generated_file, gguf_path)\n",
        "    \n",
        "    print(f\"‚úÖ GGUF model saved as: {gguf_filename}\")\n",
        "    \n",
        "    # Check file size\n",
        "    if os.path.exists(gguf_path):\n",
        "        size_mb = os.path.getsize(gguf_path) / (1024 * 1024)\n",
        "        print(f\"   File size: {size_mb:.2f} MB\")\n",
        "    \n",
        "    # Clean up the output directory if it exists\n",
        "    if os.path.exists(gguf_dir):\n",
        "        shutil.rmtree(gguf_dir, ignore_errors=True)\n",
        "else:\n",
        "    print(\"‚ùå Error: GGUF file not found after export\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Download the GGUF Model for Ollama\n",
        "\n",
        "**Note:** Downloaded file is around 2GB so it takes some time until the browser save dialog appears!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the GGUF file to your local machine\n",
        "from google.colab import files\n",
        "\n",
        "# Download the quantized model\n",
        "print(f\"Downloading GGUF model: {gguf_filename}\")\n",
        "files.download(gguf_path)\n",
        "print(\"‚úÖ Download complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 15: Create Ollama Modelfile\n",
        "\n",
        "**Note:** Browser save dialog appears, save the file to the same folder as the GGUF file!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a Modelfile for Ollama with the correct chat template\n",
        "print(\"Generating Modelfile for Ollama...\")\n",
        "\n",
        "# Detect model type from model name\n",
        "model_type = None\n",
        "if \"qwen\" in model_name.lower():\n",
        "    model_type = \"qwen\"\n",
        "elif \"llama\" in model_name.lower():\n",
        "    model_type = \"llama\"\n",
        "elif \"phi\" in model_name.lower():\n",
        "    model_type = \"phi\"\n",
        "\n",
        "print(f\"Detected model type: {model_type or 'unknown'}\")\n",
        "print(f\"Using GGUF file: {gguf_filename}\")\n",
        "\n",
        "# Generate appropriate template based on model type\n",
        "if model_type == \"qwen\":\n",
        "    # Qwen uses ChatML format\n",
        "    modelfile_content = f'''FROM ./{gguf_filename}\n",
        "\n",
        "TEMPLATE \"\"\"<|im_start|>system\n",
        "{{{{ .System }}}}<|im_end|>\n",
        "<|im_start|>user\n",
        "{{{{ .Prompt }}}}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.1\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER stop \"<|im_start|>\"\n",
        "PARAMETER stop \"<|im_end|>\"\n",
        "\n",
        "SYSTEM \"\"\"You are a helpful UI design assistant.\"\"\"\n",
        "'''\n",
        "    print(\"‚úÖ Using Qwen/ChatML template\")\n",
        "\n",
        "elif model_type == \"llama\":\n",
        "    # Llama 3.2 uses Llama 3 format with special tokens\n",
        "    modelfile_content = f'''FROM ./{gguf_filename}\n",
        "\n",
        "TEMPLATE \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{{{ .System }}}}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{{{ .Prompt }}}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.1\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER stop \"<|eot_id|>\"\n",
        "\n",
        "SYSTEM \"\"\"You are a helpful UI design assistant.\"\"\"\n",
        "'''\n",
        "    print(\"‚úÖ Using Llama 3.2 template\")\n",
        "\n",
        "elif model_type == \"phi\":\n",
        "    # Phi uses a similar format to ChatML\n",
        "    modelfile_content = f'''FROM ./{gguf_filename}\n",
        "\n",
        "TEMPLATE \"\"\"<|system|>\n",
        "{{{{ .System }}}}<|end|>\n",
        "<|user|>\n",
        "{{{{ .Prompt }}}}<|end|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.1\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER stop \"<|end|>\"\n",
        "\n",
        "SYSTEM \"\"\"You are a helpful UI design assistant.\"\"\"\n",
        "'''\n",
        "    print(\"‚úÖ Using Phi template\")\n",
        "\n",
        "else:\n",
        "    # Generic fallback template\n",
        "    print(\"‚ö†Ô∏è Warning: Unknown model type, using generic template\")\n",
        "    modelfile_content = f'''FROM ./{gguf_filename}\n",
        "\n",
        "TEMPLATE \"\"\"{{{{ .System }}}}\n",
        "\n",
        "User: {{{{ .Prompt }}}}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.1\n",
        "PARAMETER top_p 0.9\n",
        "\n",
        "SYSTEM \"\"\"You are a helpful UI design assistant.\"\"\"\n",
        "'''\n",
        "\n",
        "# Save Modelfile\n",
        "with open(\"Modelfile\", \"w\") as f:\n",
        "    f.write(modelfile_content)\n",
        "\n",
        "print(\"\\n‚úÖ Modelfile created successfully!\")\n",
        "print(\"\\nModelfile content:\")\n",
        "print(modelfile_content)\n",
        "\n",
        "# Download the Modelfile\n",
        "from google.colab import files\n",
        "files.download(\"Modelfile\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 16: Instructions for Using with Ollama\n",
        "\n",
        "After downloading the GGUF file and `Modelfile`, follow these steps on your local machine:\n",
        "\n",
        "```bash\n",
        "# 1. Make sure the GGUF file and Modelfile are in the same directory, go to that directory\n",
        "\n",
        "# 2. Create the Ollama model (you can use base model name instead of `-model` to compare more finetuned variants)\n",
        "ollama create ngui-finetuned-model -f Modelfile\n",
        "\n",
        "# 3. Run the model\n",
        "ollama run ngui-finetuned-model\n",
        "\n",
        "# 4. Test it with a prompt\n",
        "# >>> What is machine learning?\n",
        "```\n",
        "\n",
        "You can also use it via the Ollama API etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "- ‚úÖ Verifying GPU availability\n",
        "- ‚úÖ Loading a 3B parameter base model (Llama-3.2-3B-Instruct or Qwen2.5-3B-Instruct)\n",
        "- ‚úÖ Adding LoRA adapters for efficient fine-tuning\n",
        "- ‚úÖ Loading and shuffling training data from multiple JSON files\n",
        "- ‚úÖ Fine-tuning the model on custom data\n",
        "- ‚úÖ Exporting to GGUF format with custom naming `NGUI-{model_basename}.{quant_method}.gguf`\n",
        "- ‚úÖ Creating a model-specific `Modelfile` for Ollama integration\n",
        "\n",
        "The fine-tuned model is now ready to use with Ollama on your local machine!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
