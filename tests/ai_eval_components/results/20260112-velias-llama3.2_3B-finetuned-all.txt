Date: 2026-01-12
Components: all with EXACT component type check
LLM: llama 3.2 3B finetuned on skill dataset files `llm_finetuning/training_data/component_selection.json` and `llm_finetuning/training_data/json_path.json`
Env: velias ThinkPad P16v (ollama accelerated on AMD GPU)

Note: 
* Accuracy is 27% better than on base LLM
* LLM reasoning in many cases for one-card and set-of-card components was like “User explicitly requested a table component …” but not any 
component was explicitly requested in user prompts. I extended training data to provide a few examples when the user explicitly requests 
a component, and how to behave if it doesn’t fit the data structure. It helped a bit, but hasn't fully fixed that problem.

Dataset items evaluated: 208
Agent errors: 65
Agent warnings: 12
System errors: 1
Dataset errors: 0
Results by component:
{
  "chart-line": {
    "num_evals": 6,
    "num_err_agent": 4,
    "num_err_system": 0,
    "num_warn_agent": 0
  },
  "image": {
    "num_evals": 40,
    "num_err_agent": 0,
    "num_err_system": 0,
    "num_warn_agent": 0
  },
  "one-card": {
    "num_evals": 68,
    "num_err_agent": 24,
    "num_err_system": 1,
    "num_warn_agent": 5
  },
  "set-of-cards": {
    "num_evals": 38,
    "num_err_agent": 31,
    "num_err_system": 0,
    "num_warn_agent": 7
  },
  "table": {
    "num_evals": 21,
    "num_err_agent": 0,
    "num_err_system": 0,
    "num_warn_agent": 0
  },
  "video-player": {
    "num_evals": 35,
    "num_err_agent": 6,
    "num_err_system": 0,
    "num_warn_agent": 0
  }
}

Performance stats [ms]:
{
  "min": 3672,
  "mean": 6697,
  "avg": 6697,
  "perc95": 10547,
  "max": 15913
}

