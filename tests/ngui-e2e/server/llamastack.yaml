version: '2'
image_name: myapp-llamastack
apis:
  - inference
server:
  tls_certfile: ${env.LIGHTRAIL_LLAMA_STACK_TLS_CERT_PATH:}
  tls_keyfile: ${env.LIGHTRAIL_LLAMA_STACK_TLS_KEY_PATH:}
providers:
  inference:
    - provider_id: vllm-inference
      provider_type: remote::vllm
      config:
        url: ${env.MYAPP_MODEL_BASE_URL}
        api_token: ${env.MYAPP_MODEL_API_TOKEN}
        tls_verify: ${env.LIGHTRAIL_LLAMA_STACK_TLS_CA_CERT_PATH}
models:
  - provider_id: vllm-inference
    model_id: ${env.NGUI_MODEL}
    model_type: llm
    metadata: {}

