# =============================================================================
# Local / direct LLM mode (Ollama, Gemini via OpenAI-compatible API, etc.)
# Use these when running the server locally without LlamaStack.
# =============================================================================
LLM_MODEL=llama3.2:3b
LLM_BASE_URL=http://localhost:11434/v1

# API Key for authenticated providers (optional)
# Required for Gemini, OpenAI, and other authenticated services
# Leave empty for local Ollama setup
LLM_API_KEY=

# =============================================================================
# LlamaStack mode (Lightrail / deployed environments)
# Use these when the server runs against a LlamaStack inference service.
# =============================================================================
# LLAMA_STACK_BASE_URL=http://localhost:5002
# NGUI_MODEL=your-model-id
# LLAMA_STACK_TLS_CA_CERT_PATH=/path/to/ca.pem
