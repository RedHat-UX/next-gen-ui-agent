# Next Gen UI MCP Server

# Build locally
# pants package ::
#
# Run:
# podman run --rm -it -p 5000:5000 --env MCP_PORT="5000" --env NGUI_MODEL="llama3.2" --env NGUI_PROVIDER_API_BASE_URL=http://host.containers.internal:11434 --env NGUI_PROVIDER_API_KEY="ollama" quay.io/next-gen-ui/mcp
FROM registry.access.redhat.com/ubi9/python-312-minimal:9.6

# Set work directory
WORKDIR /opt/app-root/src

# Set environment variables for build
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1

# Copy Python Project Files (Container context must be the `python` directory)
COPY --chown=1001:root . /opt/app-root/src/

USER root

# Install dependencies
RUN pip install *.whl \
    langchain_openai

# Allow non-root user to access the everything in app-root
RUN chgrp -R root /opt/app-root/src && chmod -R g+rwx /opt/app-root/src

ENV MCP_TRANSPORT="streamable-http"
ENV MCP_HOST="0.0.0.0"
ENV MCP_PORT="5000"
ENV NGUI_COMPONENT_SYSTEM="json"
ENV NGUI_PROVIDER="langchain"
ENV NGUI_PROVIDER_API_KEY=""
ENV NGUI_PROVIDER_API_BASE_URL=""
ENV NGUI_MODEL="gpt-4o"
ENV NGUI_PROVIDER_LLAMA_URL=""
ENV NGUI_CONFIG_PATH="-"

# Expose default port (change if needed)
EXPOSE $MCP_PORT

USER 1001

# Run the MCP
# TODO: Support natively env variables in the next_gen_ui_mcp module
CMD python -m next_gen_ui_mcp --config-path $NGUI_CONFIG_PATH \
    --host "$MCP_HOST" --port "$MCP_PORT" --transport "$MCP_TRANSPORT" --component-system "$NGUI_COMPONENT_SYSTEM" \
    --provider "$NGUI_PROVIDER" --model "$NGUI_MODEL" --api-key "$NGUI_PROVIDER_API_KEY" --base-url "$NGUI_PROVIDER_API_BASE_URL" \
    --llama-url "$NGUI_PROVIDER_LLAMA_URL"
