from abc import ABC, abstractmethod
from typing import Any, Optional

from next_gen_ui_agent.model import InferenceBase
from pydantic import BaseModel, ConfigDict, Field
from typing_extensions import NotRequired, TypedDict


# Intentionaly TypeDict because of passing ABC class InferenceBase
class AgentConfig(TypedDict):
    """Agent Configuration."""

    inference: NotRequired[InferenceBase]
    """Inference to use to call LLM by the agent."""

    component_system: NotRequired[str | None]
    """Component system to use to render the component."""

    unsupported_components: NotRequired[bool | None]
    """
    If `False` (default), the agent can generate only fully supported UI components.
    If `True`, the agent can also generate unsupported UI components.
    """

    component_selection_strategy: NotRequired[str | None]
    """
    Component selection strategy to use.
    Possible values:
    - default - use the default implementation
    - one_llm_call - use the one LLM call implementation from component_selection.py
    - two_llm_calls - use the two LLM calls implementation from component_selection_twostep.py
    """

    hand_build_components_mapping: NotRequired[dict[str, str] | None]
    """
    Mapping from `InputData.type` to hand-build `component_type` (aka HBC).
    LLM powered component selection and configuration is skipped for HBC, data are propagated "as is", and only
    rendering is performed by hand-build code registered in the renderer for given `component_type`.
    """

    input_data_json_wrapping: NotRequired[bool | None]
    """
    If `True` (default), the agent will wrap the JSON input data into data type field if necessary due to its structure.
    If `False`, the agent will never wrap the JSON input data into data type field.
    """


class InputData(TypedDict):
    """Agent Input Data."""

    id: str
    """
    ID of the input data so we can reference them during the agent processing.
    Must be unique for each `InputData` in one UI Agent call (in one `AgentInput`).
    """
    data: str
    """JSON data to be processed."""
    type: NotRequired[str | None]
    """
    Optional type identification of the input data. Used for "hand-build component" selection
    based on Agent's configuration. See `AgentConfig.hand_build_components_mapping`.
    """
    hand_build_component_type: NotRequired[str | None]
    """
    Optional `component_type` of the "hand-build component" to be used for UI rendering.
    """


class InputDataInternal(InputData):
    """Input Data used in internal call of component selection. Contain parsed JSON data."""

    json_data: Any


class AgentInput(TypedDict):
    """Agent Input."""

    user_prompt: str
    """User prompt to be processed."""
    input_data: list[InputData]
    """Input data to be processed - one or more can be provided."""


class DataField(BaseModel):
    """UI Component field metadata."""

    model_config = ConfigDict(title="RenderContextDataField")

    name: str = Field(description="Field name")
    data_path: str = Field(description="JSON Path to input data")
    """JSON Path to input data"""


class UIComponentMetadata(BaseModel):
    """UI Component Mentadata."""

    id: Optional[str] = None
    """ID of the data this instance is for."""
    title: str
    """Title of the component."""
    reasonForTheComponentSelection: Optional[str] = None
    """Reason for the component selection generated by LLM."""
    confidenceScore: Optional[str] = None
    """Confidence score of the component selection generated by LLM."""
    component: str
    """Component type."""
    fields: list[DataField]
    """Fields of the component."""

    json_data: Optional[Any] = None
    """
    Parsed JSON data that was used to generate the component configuration.
    May be altered against original InputData by json wrapping!
    """


class UIComponentMetadataHandBuildComponent(UIComponentMetadata):
    """UI Component Mentadata for hand-build component."""

    component_type: str
    """Type of the hand-build component."""


class Rendition(BaseModel):
    """Rendition of the component."""

    id: str
    component_system: str
    mime_type: str
    content: str


class ComponentSelectionStrategy(ABC):
    """Abstract base class for LLM-based component selection and configuration strategies."""

    input_data_json_wrapping: bool
    """
    If `True`, the agent will wrap the JSON input data into data type field if necessary due to its structure.
    If `False`, the agent will never wrap the JSON input data into data type field.
    """

    def __init__(self, input_data_json_wrapping: bool):
        self.input_data_json_wrapping = input_data_json_wrapping

    @abstractmethod
    async def select_components(
        self, inference: InferenceBase, input: AgentInput
    ) -> list[UIComponentMetadata]:
        """
        Select UI components based on input data and user prompt.
        Args:
            inference: Inference to use to call LLM by the agent
            input: Agent input
            json_data: optional JSON data parsed into python objects to be processed
        Returns:
            List of `UIComponentMetadata`
        """
        pass

    @abstractmethod
    async def perform_inference(
        self,
        inference: InferenceBase,
        user_prompt: str,
        json_data: Any,
        input_data_id: str,
    ) -> list[str]:
        """
        Perform inference to select UI components and configure them.
        Multiple LLM calls can be performed and inference results can be returned as a list of strings.

        Args:
            inference: Inference to use to call LLM by the agent
            user_prompt: User prompt to be processed
            json_data: JSON data parsed into python objects to be processed
            input_data_id: ID of the input data

        Returns:
            List of strings with LLM inference outputs
        """
        pass

    @abstractmethod
    def parse_infernce_output(
        self, inference_output: list[str], input_data_id: str
    ) -> UIComponentMetadata:
        """
        Parse LLM inference outputs from `perform_inference` and return `UIComponentMetadata`
        or throw exception if it can't be constructed because of invalid LLM outputs.

        Args:
            inference_output: List of strings with LLM inference outputs
            input_data_id: ID of the input data

        Returns:
            `UIComponentMetadata`
        """
        pass
