# GitLab CI/CD Pipeline for LLM Evaluation Tests
# This mirrors the GitHub Actions workflow but runs on GitLab

default:
  tags:
    - ${GITLAB_RUNNER_TAGS}

workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "web"
      when: always
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: always
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: always
    - when: never

variables:
  PYTHON_VERSION: "3.13"
  PYTHONPATH: "tests:libs"
  DEBIAN_FRONTEND: "noninteractive"
  KUBERNETES_CPU_REQUEST: "1"
  KUBERNETES_CPU_LIMIT: "2"
  KUBERNETES_MEMORY_REQUEST: "2Gi"
  KUBERNETES_MEMORY_LIMIT: "4Gi"

stages:
  - test
  - report
  - aggregate
  - deploy

complete_eval:
  stage: test
  image: ${REGISTRY_URL}/ubuntu:latest
  tags:
    - ${GITLAB_RUNNER_TAGS}
  timeout: 20 hours
  rules:
    - if: $AGGREGATE_JOB_IDS
      when: never
    - when: on_success
  before_script:
    - echo "=== Setting up environment ==="
    - apt-get update
    - apt-get install -y software-properties-common curl
    - echo "=== Installing Python 3.13 ==="
    - add-apt-repository -y ppa:deadsnakes/ppa
    - apt-get update
    - apt-get install -y python3.13 python3.13-venv python3.13-dev python3-pip
    - python3.13 --version
    - echo "=== Installing Python dependencies ==="
    - python3.13 -m pip install --break-system-packages -r libs/3rdparty/python/agent-core-requirements.txt
    - python3.13 -m pip install --break-system-packages -r libs/3rdparty/python/langchain-requirements.txt
    - python3.13 -m pip install --break-system-packages -r libs/3rdparty/python/llama-stack-requirements.txt
    - python3.13 -m pip install --break-system-packages -c libs/3rdparty/python/llama-stack-client-constraints.txt -r libs/3rdparty/python/llama-stack-client-requirements.txt
    - mkdir -p ~/.llama/providers.d tests/ai_eval_components/errors tests/ai_eval_components/errors_k8s
    - echo "=== DEBUG - Checking CI/CD Variables ==="
    - echo "MODEL_API_URL is set to '${MODEL_API_URL}'"
    - echo "INFERENCE_MODEL is set to '${INFERENCE_MODEL}'"
    - |
      if [ -z "$MODEL_API_URL" ] || [ -z "$INFERENCE_MODEL" ] || [ -z "$MODEL_API_KEY" ]; then
        echo "ERROR: Required CI/CD variables not set!"
        echo "Please set: MODEL_API_URL, INFERENCE_MODEL, MODEL_API_KEY"
        exit 1
      fi
    - echo "All required CI/CD variables are set"
  script:
    - echo "=== Running Movies/Subscriptions Tests ==="
    - >
      python3.13 -c "import sys; sys.argv.extend(['-c', 'all', '-w']);
      import httpx; o=httpx.AsyncClient.__init__;
      httpx.AsyncClient.__init__=lambda s,*a,**k: o(s,*a,**{**k,'verify':False});
      import aiohttp; orig_init=aiohttp.ClientSession.__init__;
      aiohttp.ClientSession.__init__=lambda s,*a,**k: orig_init(s,*a,**{**k,'connector':aiohttp.TCPConnector(ssl=False)});
      import runpy; runpy.run_module('ai_eval_components.eval', run_name='__main__')"
    - echo "=== Running Kubernetes Tests ==="
    - export ERRORS_DIR=tests/ai_eval_components/errors_k8s
    - export LLM_OUT_DIR=tests/ai_eval_components/llm_out_k8s
    - >
      python3.13 -c "import sys; sys.argv.extend(['-c', 'all', '-w']);
      import httpx; o=httpx.AsyncClient.__init__;
      httpx.AsyncClient.__init__=lambda s,*a,**k: o(s,*a,**{**k,'verify':False});
      import aiohttp; orig_init=aiohttp.ClientSession.__init__;
      aiohttp.ClientSession.__init__=lambda s,*a,**k: orig_init(s,*a,**{**k,'connector':aiohttp.TCPConnector(ssl=False)});
      import runpy; runpy.run_module('ai_eval_components.eval_k8s', run_name='__main__')"
    - echo "=== Generating Report ==="
    - python3.13 -m ai_eval_components.generate_report --datasets dataset,dataset_k8s --labels "Movies/Subscriptions,Kubernetes" --error-dirs errors,errors_k8s --llm-out-dirs tests/ai_eval_components/llm_out,tests/ai_eval_components/llm_out_k8s --title "GitLab CI Evaluation Report" --model "${INFERENCE_MODEL}" --output tests/ai_eval_components/gitlab_eval_report.html
    - echo "=== Completed ==="
  artifacts:
    when: always
    paths:
      - tests/ai_eval_components/gitlab_eval_report.html
      - tests/ai_eval_components/errors/
      - tests/ai_eval_components/errors_k8s/
      - tests/ai_eval_components/llm_out/
      - tests/ai_eval_components/llm_out_k8s/
      - tests/ai_eval_components/perf_stats.json
      - tests/ai_eval_components/perf_stats_k8s.json
    expire_in: 30 days

# ============================================
# REPORT METADATA EXTRACTION
# ============================================

generate_report_metadata:
  stage: report
  image: python:3.13
  rules:
    - if: $AGGREGATE_JOB_IDS
      when: never
    - when: on_success
  dependencies:
    - complete_eval
  before_script:
    - export PYTHONPATH="tests:libs"
  script:
    - python -m ai_eval_components.save_report_metadata
  artifacts:
    when: always
    paths:
      - tests/ai_eval_components/report_metadata.json
    expire_in: 1 year

# ============================================
# DASHBOARD AGGREGATION (Manual Mode Only)
# ============================================

aggregate_dashboard:
  stage: aggregate
  image: python:3.13
  rules:
    - if: $AGGREGATE_JOB_IDS
      when: always
    - when: never
  before_script:
    - pip install requests
    - export PYTHONPATH="tests:libs"
  script:
    - |
      TOKEN="${AGGREGATION_TOKEN:-$CI_JOB_TOKEN}"
      python -m ai_eval_components.aggregate_reports \
        --project-id $CI_PROJECT_ID \
        --gitlab-url $CI_SERVER_URL \
        --token "$TOKEN" \
        --job-ids "$AGGREGATE_JOB_IDS" \
        --output public
    - python -m ai_eval_components.generate_dashboard --input public --output public/index.html
    - |
      cat > public/_headers << 'EOF'
      /*
        Cache-Control: no-store, no-cache, must-revalidate, max-age=0
        Pragma: no-cache
        Expires: 0
      EOF
  artifacts:
    paths:
      - public/

# ============================================
# GITLAB PAGES DEPLOYMENT
# ============================================

pages:
  stage: deploy
  needs:
    - job: aggregate_dashboard
      optional: true
  rules:
    - if: $AGGREGATE_JOB_IDS
      when: always
  script:
    - echo "Deploying evaluation dashboard to GitLab Pages"
    - ls -la public/ || echo "No public directory found"
  artifacts:
    paths:
      - public/
