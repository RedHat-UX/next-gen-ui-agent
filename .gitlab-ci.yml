# GitLab CI/CD Pipeline for LLM Evaluation Tests
# This mirrors the GitHub Actions workflow but runs on GitLab
# Uses Ollama for testing (same as GitHub Actions)

default:
  tags:
    - shared-podman

workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "web"
      when: always
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: always
    - if: $CI_COMMIT_BRANCH == "test_ai_eval"
      when: always
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: always
    - when: never

variables:
  PYTHON_VERSION: "3.13"
  INFERENCE_MODEL: "llama3.2:3b"
  PYTHONPATH: "tests:libs"
  DEBIAN_FRONTEND: "noninteractive"

stages:
  - test

complete_eval:
  stage: test
  image: ubuntu:latest
  tags:
    - shared-podman
  timeout: 20 hours
  before_script:
    - echo "=== Setting up environment ==="
    - apt-get update
    - apt-get install -y software-properties-common curl
    - echo "=== Installing Python 3.13 ==="
    - add-apt-repository -y ppa:deadsnakes/ppa
    - apt-get update
    - apt-get install -y python3.13 python3.13-venv python3.13-dev python3-pip
    - python3.13 --version
    - echo "=== Installing Python dependencies ==="
    - python3.13 -m pip install --break-system-packages -r libs/3rdparty/python/agent-core-requirements.txt
    - python3.13 -m pip install --break-system-packages -r libs/3rdparty/python/langchain-requirements.txt
    - python3.13 -m pip install --break-system-packages -r libs/3rdparty/python/llama-stack-requirements.txt
    - python3.13 -m pip install --break-system-packages -c libs/3rdparty/python/llama-stack-client-constraints.txt -r libs/3rdparty/python/llama-stack-client-requirements.txt
    - echo "=== Installing Ollama ==="
    - curl -fsSL https://ollama.com/install.sh | sh
    - echo "=== Starting Ollama ==="
    - ollama serve &
    - sleep 5
    - for i in {1..30}; do if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then echo "Ollama ready!" && break; fi; sleep 2; done
    - ollama pull $INFERENCE_MODEL
    - mkdir -p ~/.llama/providers.d tests/ai_eval_components/errors tests/ai_eval_components/errors_k8s
  script:
    - echo "=== Running Movies/Subscriptions Tests ==="
    - python3.13 -m ai_eval_components.eval
    - echo "=== Running Kubernetes Tests ==="
    - python3.13 -m ai_eval_components.eval_k8s
    - echo "=== Generating Report ==="
    - python3.13 -m ai_eval_components.generate_report --datasets dataset,dataset_k8s --labels "Movies/Subscriptions,Kubernetes" --error-dirs errors,errors_k8s --title "GitLab CI Evaluation Report" --output tests/ai_eval_components/gitlab_eval_report.html
    - echo "=== Completed ==="
  artifacts:
    when: always
    paths:
      - tests/ai_eval_components/gitlab_eval_report.html
      - tests/ai_eval_components/errors/
      - tests/ai_eval_components/errors_k8s/
      - tests/ai_eval_components/perf_stats.json
      - tests/ai_eval_components/perf_stats_k8s.json
    expire_in: 30 days
